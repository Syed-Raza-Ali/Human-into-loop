{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+F7wkKwi2IaXZCY/0Xhna",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syed-Raza-Ali/Human-into-loop/blob/main/human_into_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**HUMAN INTO LOOP**\n",
        "- **Under Construction**"
      ],
      "metadata": {
        "id": "wRJ1wy9g106P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all necessary packages\n",
        "!pip install -q langchain-core langchain-google-genai langgraph python-dotenv tavily-python langchain_community"
      ],
      "metadata": {
        "id": "SbTwuTRb2GRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API KEYS VARIABLES\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "hW9onsJE2Ufy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "\n",
        "# Initialize Tavily search tool\n",
        "tavily_search = TavilySearchResults(max_results = 3)\n",
        "\n",
        "# Maintain chat history\n",
        "chat_history = []\n",
        "\n",
        "def stream_graph_updates(user_input):\n",
        "    \"\"\"A placeholder function for processing graph updates.\"\"\"\n",
        "    print(f\"Processing graph updates for: {user_input}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "  \"\"\" Get realtime information using tavily search \"\"\"\n",
        "\n",
        "  while True:\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Prompt the user for input\n",
        "        print(\"\\n\")\n",
        "        user_input = input(\"User : \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\",\"bye\"]:\n",
        "            print(\"Bye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Handle empty input\n",
        "        if not user_input:\n",
        "            print(\"No prompt found. Please type something.\")\n",
        "            continue\n",
        "\n",
        "        # Add user input to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Call the placeholder function (replace as needed)\n",
        "        try:\n",
        "            stream_graph_updates(user_input)\n",
        "        except NameError:\n",
        "            # Handle missing function gracefully\n",
        "            print(\"stream_graph_updates is not implemented. Skipping.\")\n",
        "\n",
        "        # Perform a Tavily search if the function is not implemented\n",
        "        search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "        # Add Tavily response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "        # Display results from Tavily search\n",
        "        print(\"Search Results:\")\n",
        "        for i, doc in enumerate(search_docs, start=1):\n",
        "            print(f\"\\nResult {i}:\")\n",
        "            print(f\"Here is the {i} documentaion from web search you can get your answer from here:\")\n",
        "            print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "            print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for message in chat_history:\n",
        "      role = message[\"role\"]\n",
        "      content = message[\"content\"]\n",
        "      print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "tBLw1raf-EY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "\n",
        "# Disable LangSmith Tracing programmatically\n",
        "langchain_api_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Health-Agent\"\n",
        "\n",
        "# Initialize the Gemini LLM model with Google API Key\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# LangGraph nodes\n",
        "def model_conversation(state: MessagesState):\n",
        "    \"\"\"Core conversational logic.\"\"\"\n",
        "    system_message = \"\"\n",
        "    messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the workflow\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"conversation\", model_conversation)\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_edge(\"conversation\", END)\n",
        "\n",
        "\n",
        "# Compile the graph with memory\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "# Chatbot runtime logic\n",
        "def agent_calling():\n",
        "    \"\"\"Run the agent.\"\"\"\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    # print(\"Hello dear, i am your MindCare Assistant, My expertise is about mental health\")\n",
        "    # print(\"Note: I have been designed exclusively as a MindCare assistant.i will not able to response any other questions\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in [\"exit\", \"bye\", \"quit\",\"q\"]:\n",
        "            print(\"Goodbye! Thank you for your trust, I hope this information is helpful for you\")\n",
        "            break\n",
        "\n",
        "\n",
        "        input_message = HumanMessage(content=user_input)\n",
        "\n",
        "        try:\n",
        "            output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        bot_response = output[\"messages\"][-1].content\n",
        "        print(f\"Chatbot: {bot_response}\")\n",
        "\n",
        "        # take snapshot for memory\n",
        "        state = graph.get_state(config)\n",
        "        # print(state)\n",
        "\n",
        "\n",
        "\n",
        "# Run the chatbot\n",
        "agent_calling()\n",
        "\n"
      ],
      "metadata": {
        "id": "PLkaV8k8-HYq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}